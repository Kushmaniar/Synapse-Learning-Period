{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Copy of Task1.ipynb","provenance":[{"file_id":"1SZqABUr3eG9jtZfAd9ukrWrgf3YlCcwa","timestamp":1632804163776},{"file_id":"104rkhBnX-dWZRNvhXt_bZ_PtX8-0uoRN","timestamp":1632739771478}],"collapsed_sections":["2ufinDlF2oH7","2tImb-hXsXmI","NzSBvjqdRfiM","V-eJsxXrTuBJ","63jou6PJaoqp","EQKtPBSSgUpN","Az401HQIi8FR","8KXM4Xe0m-bj"]},"kernelspec":{"name":"python3","display_name":"Python 3.9.7 64-bit"},"language_info":{"name":"python","version":"3.9.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"}},"cells":[{"cell_type":"markdown","source":["# Task 1\n","#### Week 1 (Learning Period Synapse)\n","###### pandas, numpy, matplotlib"],"metadata":{"id":"C8KlsCFL1tvs"}},{"cell_type":"code","execution_count":469,"source":["from google.colab import drive\r\n","drive.mount(\"/content/gdrive\")"],"outputs":[],"metadata":{"id":"wcGii-p723dl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632892878828,"user_tz":-330,"elapsed":1242,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"668c9393-d259-4610-e507-0c9aadd026de"}},{"cell_type":"code","execution_count":null,"source":["!pip install matplotlib\r\n","!pip install jupyterthemes"],"outputs":[],"metadata":{"id":"m6HJFwYI9U55","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632812088785,"user_tz":-330,"elapsed":4648,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"346aa291-c907-44dc-f5de-f93560c12f56"}},{"cell_type":"markdown","source":["## 1. Import pandas, numpy and matplotlib"],"metadata":{"id":"2ufinDlF2oH7"}},{"cell_type":"code","execution_count":471,"source":["#write code here\r\n","import numpy as np\r\n","import pandas as pd\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns"],"outputs":[],"metadata":{"id":"V0KqGm1I2mx6","executionInfo":{"status":"ok","timestamp":1632893322525,"user_tz":-330,"elapsed":598,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}}}},{"cell_type":"markdown","source":["## 2. load the data \n","<br> load the titanic dataset in a pandas dataframe <br> (download dataset from https://www.kaggle.com/c/titanic/data and store it in a folder called <b>\"syanpse_w1\"</b> in the root of your drive) <br>\n","Note that you have to download the train and test csv files seperately"],"metadata":{"id":"5dFTZvpm28n7"}},{"cell_type":"code","execution_count":472,"source":["\r\n","train_path = \"train.csv\"\r\n","test_path = \"test.csv\"\r\n","\r\n","#write code to load train.csv and test.csv in dataframes\r\n","df = pd.read_csv(train_path)\r\n","test_df = pd.read_csv(test_path)"],"outputs":[],"metadata":{"id":"mY_Fol-u1qmi","executionInfo":{"status":"ok","timestamp":1632893615052,"user_tz":-330,"elapsed":362,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}}}},{"cell_type":"code","execution_count":null,"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"outputs":[],"metadata":{"id":"W8nML4FVXDh3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632893617210,"user_tz":-330,"elapsed":365,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"0d405ab4-31aa-4cf6-d672-f8718c61bffc"}},{"cell_type":"code","execution_count":null,"source":["#write code to show first five data entries of both dataframes\r\n","fivetrain_df = pd.read_csv(train_path, nrows=5)\r\n","\r\n","\r\n","fivetest_df = pd.read_csv(test_path, nrows=5)\r\n","\r\n","print(fivetrain_df)\r\n","print(fivetest_df)\r\n"],"outputs":[],"metadata":{"id":"loyfFwFK1s6-","executionInfo":{"status":"ok","timestamp":1632893619845,"user_tz":-330,"elapsed":644,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}}}},{"cell_type":"markdown","source":["## 3. Exploratory Data Analysis\n","Exploratory data analysis is a very important part of ML as it helps you understand the data you are dealing with <br>\n","<br>\n","(we will be working on the train dataframe in this task)"],"metadata":{"id":"eAe9H4ub-9TB"}},{"cell_type":"markdown","source":["### 3.a. overview of the dataset"],"metadata":{"id":"7n5NzvIOAH1G"}},{"cell_type":"markdown","source":[" - write code to list the columns in your dataframe (each column on new line as visibility is important)"],"metadata":{"id":"dIl7EiGEBjPL"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","list=df.columns\r\n","print(list)\r\n"],"outputs":[],"metadata":{"id":"jgGpmmk4BNpf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632893517380,"user_tz":-330,"elapsed":338,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"08c8ca0d-d77f-44bc-e523-6ed11b6957ba"}},{"cell_type":"markdown","source":[" - write a function that returns the number of rows and columns in the dataset "],"metadata":{"id":"BbtlcScEBkwG"}},{"cell_type":"code","execution_count":null,"source":["#write code here (print values as well)\r\n","\r\n","rows = len(df.axes[0])\r\n","cols = len(df.axes[1])\r\n","print(\"Number of Rows: \" , rows)\r\n","print(\"Number of Cols: \" , cols)\r\n"],"outputs":[],"metadata":{"id":"huYZDIdW87BM","colab":{"base_uri":"https://localhost:8080/","height":217},"executionInfo":{"status":"error","timestamp":1632893624207,"user_tz":-330,"elapsed":364,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"d134996f-fb38-4335-9d48-0c70e61ba0ea"}},{"cell_type":"markdown","source":[" - use the info() method of pandas dataframe to print the info related to the dataframe"],"metadata":{"id":"tFWvBxGvCR5M"}},{"cell_type":"code","execution_count":null,"source":["# write code here\r\n","\r\n","df.info(verbose=False)"],"outputs":[],"metadata":{"id":"oDGygTjuAhtf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632893536731,"user_tz":-330,"elapsed":408,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"5a5075cf-d862-499c-8de6-a770abfc4d20"}},{"cell_type":"markdown","source":[" - use the describe() method of pandas dataframe to print the statistical description of the dataset"],"metadata":{"id":"JtEy2fQuCfic"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","\r\n","#perc =[.20, .40, .60, .80]\r\n","#include =['object', 'float', 'int']\r\n","df.describe()"],"outputs":[],"metadata":{"id":"HmAzuSpcCfAC","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1632893540976,"user_tz":-330,"elapsed":343,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"b5051507-9ebb-4877-cb57-2f0b95067dca"}},{"cell_type":"markdown","source":[" - Find all columns that have unique values. (example : the Gender column has 2 unique values Male and Female) <br>\n","\n"," - if the columns have finite unique values, then add them in a dictionary of lists and print it <br>\n","<br>\n","dictionary should look like : <br>\n","<t>{<br>&nbsp;&nbsp; \"Gender\" : [\"Male\", \"Female\"],<br>&nbsp;&nbsp; \"Col2\" : [\"unique1\", \"unique2\"],<br>&nbsp;&nbsp;&nbsp;&nbsp;.<br>&nbsp;&nbsp;&nbsp;&nbsp;.<br>&nbsp;&nbsp;&nbsp;&nbsp;.<br>}"],"metadata":{"id":"A9e7GcsJC_ww"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","i=0\r\n","dicts = {}\r\n","keys = df.columns\r\n","for col in df.columns:\r\n","  lists = df[col].unique()\r\n","  if len(lists) < 10:\r\n","    dicts[keys[i]] = lists\r\n","  i += 1\r\n","for key, value in dicts.items():\r\n","  print(key, \":\", value)"],"outputs":[],"metadata":{"id":"SvWOT_pKDbdu","colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"status":"error","timestamp":1632893712573,"user_tz":-330,"elapsed":349,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"68e290ac-89ff-4441-cbb2-783791eede69"}},{"cell_type":"markdown","source":["### 3.b. Visualizing hidden Details"],"metadata":{"id":"V1OdlEqXHzMC"}},{"cell_type":"markdown","source":["##### Aim is to find relationship between Title and Survival rate\n","- print the <b>name</b> column of the data<br>\n","- notice that the word after the first comma encapsulates the title of the person (Mr., Mrs, etc)\n","- extract the title using regular expression (help given below in the code)\n","- store the title in a new column in the dataframe. Name the column : <b>Title</b>"],"metadata":{"id":"De37jtciIY7F"}},{"cell_type":"code","execution_count":null,"source":["#write code here (for regex help refer next code cell)\r\n","import re\r\n","\r\n","\r\n","#new_df = pd.DataFrame()\r\n","#tit = []\r\n","for index, row in df.iterrows():\r\n","    name_eg =row['Name']\r\n","    title_search = re.search('(\\w+)\\.', name_eg) \r\n","    if title_search:\r\n","      title = title_search.group(1) \r\n","      #tit.append(title)\r\n","      #new_df[\"Title\"]=pd.Series(tit)\r\n","    print(title)\r\n","    \r\n"],"outputs":[],"metadata":{"id":"URnLEQP-EsXA"}},{"cell_type":"code","execution_count":null,"source":["#regex example\r\n","import re\r\n","\r\n","name_eg = \"Petranec, Mr. Matilda\"\r\n","title_search = re.search('(\\w+)\\.', name_eg) #the first argument is a pattern that we are looking for\r\n","print(\"result:\", title_search) #this is the result we get after looking for the pattern\r\n","if title_search:\r\n","  print(\"pattern found\") #pattern found printed if a pattern like above is found\r\n","  title = title_search.group(1) #extracting the pattern found by regular expression (title here)\r\n","  print(\"title:\", title) "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X62p2RoZIp6U","executionInfo":{"status":"ok","timestamp":1632812093458,"user_tz":-330,"elapsed":5,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"3d212183-b906-4584-dc35-e1a1ac18129f"}},{"cell_type":"code","execution_count":null,"source":["#show the new dataframe with \"Title\" column here\r\n","import re\r\n","\r\n","\r\n","tit=[]\r\n","for index, row in df.iterrows():\r\n","    name_eg =row['Name']\r\n","    title_search = re.search('(\\w+)\\.', name_eg) \r\n","    if title_search:\r\n","      title = title_search.group(1) \r\n","      tit.append(title)\r\n","\r\n","df2 = pd.DataFrame(tit, columns=['Title'])\r\n","print (df2)"],"outputs":[],"metadata":{"id":"5ZMsfu5HKYbU"}},{"cell_type":"markdown","source":["- write code to get unique values from the title column\n","- use the unique titles to find the number of people who survived (1) and number of people who did not (0) for each title. <b>num_survived/total</b> will give the survival rate for that title\n","- make a new dataframe called <b>SR_df</b> and add the survival rates corresponding to titles in it<br><br>\n","you may write a function for the above.<br><br>\n","expected output : <br>\n","Mr &nbsp;&nbsp; 0.679392<br>\n","Capt &nbsp;&nbsp; 0.1245<br>\n","&nbsp;&nbsp;.<br>\n","&nbsp;&nbsp;.<br>\n","&nbsp;&nbsp;.<br>\n"],"metadata":{"id":"xQBmJI2eM5KT"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","df['Title']=df2['Title']\r\n","unique= df['Title'].unique()\r\n","dict={}\r\n","\r\n","for i in unique :\r\n","  count=k=0\r\n","  survived=0\r\n","  for j in df['Title']:\r\n","    if j==i :\r\n","      count +=1\r\n","      if df.at[k,\"Survived\"]==1:\r\n","        survived+=1\r\n","    k+=1\r\n","  dict[i]= survived/count\r\n","\r\n","for key, value in dict.items():\r\n","  print(key, \":\", value)\r\n","\r\n","\r\n"],"outputs":[],"metadata":{"id":"HYHZxu-YO-NJ","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1632890750371,"user_tz":-330,"elapsed":760,"user":{"displayName":"Kush Maniar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9W-FNtwto7bZCMD0fh_O3udaWKSsEaGsrR-8JQ=s64","userId":"12110693652135874299"}},"outputId":"bf0d2410-47ee-49a0-e53a-2e0565f3efa0"}},{"cell_type":"code","execution_count":null,"source":["#show the new dataframe SR_df here\r\n","SR_df=pd.DataFrame(dict.items(), columns=['Title','Survival Rate'])\r\n","SR_df"],"outputs":[],"metadata":{"id":"eftv29NFPJzr"}},{"cell_type":"markdown","source":["- use SR_df to plot a bar graph for survival rate vs Title"],"metadata":{"id":"Olnh97qqO-tZ"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","SR_df.plot.bar(x=\"Title\" ,y=\"Survival Rate\")\r\n","plt.ylabel(\"Survival Rate\", labelpad=14)\r\n","plt.xlabel(\"Title\", labelpad=14)"],"outputs":[],"metadata":{"id":"QYmrVku9MPdT"}},{"cell_type":"markdown","source":["This brings us to the end of EDA. <br>\n","You may feel free to do more exploratry analysis.<br>\n","The aim of this notebook is not to point out every small detail but to give you an overview and guide you"],"metadata":{"id":"xrIVW_mHQPJX"}},{"cell_type":"markdown","source":["## 4. Data Visualization"],"metadata":{"id":"2tImb-hXsXmI"}},{"cell_type":"markdown","source":["- Make a pie chart to show the number of people who survived and number of people who did not\n","- Make a pie chart to show the number of <b>Males</b> who survived and number of people who did not\n","- Make a pie chart to show the number of <b>Females</b> who survived and number of people who did not\n","<br><br>\n","Note that all three charts must be visible in a single row (look into subplots)"],"metadata":{"id":"ah1dARlJsddI"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","s_male=s_female=0\r\n","d_male=d_female=0\r\n","s=d=0\r\n","i=0\r\n","\r\n","while (i<rows):\r\n","    if (df.at[i,'Sex'] in ['male']):\r\n","        if (df2.at[i,'Survived']==1):\r\n","            s_male+=1    \r\n","        else:\r\n","            d_male+=1   \r\n","    else:\r\n","        if(df.at[i,'Survived']==1):\r\n","            s_female+=1    \r\n","        else:\r\n","            d_female+=1         \r\n","    i+=1\r\n","s=s_male+s_female\r\n","d=d_male+d_female\r\n","total=[s, d]\r\n","male=[s_male, d_male]\r\n","female=[d_female, s_female]\r\n","totlabels = [\"total survived\", \"total dead\"]\r\n","malelabel=[\"male survived\", \"male dead\"]\r\n","femalabel=[\"female dead\", \"female survived\"]\r\n","plt.subplot(1,10,1,)\r\n","plt.pie(total, labels = totlabels, radius =5)\r\n","plt.subplot(1,10,5)\r\n","plt.pie(male, labels = malelabel, radius =5)\r\n","plt.subplot(1,10,10)\r\n","plt.pie(female, labels = femalabel, radius =5)\r\n","plt.subplots_adjust(left=0.1,bottom=0.1, right=0.9, top=0.9, wspace=1, hspace=0.5)\r\n","plt.show()\r\n"],"outputs":[],"metadata":{"id":"gaZyWEN5sXP6"}},{"cell_type":"markdown","source":["- Find if money affected survival rate (fare and Pclass) \n"," - Make a bar graph showing Pclass vs survival rate\n"," - Make a KDE plot (use the sns library) and plot Fare based on the Survival (google away for this one)"],"metadata":{"id":"uYSmxfqEsjGo"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","s1=d1=s2=d2=s3=d3=0\r\n","sr1=sr2=sr3=0\r\n","i=0\r\n","pclassdf=df['Pclass'].unique()\r\n","pclassdf.sort()\r\n","\r\n","while (i<rows):\r\n","    if (df.at[i,'Pclass'] ==1):\r\n","        \r\n","        if (df.at[i,'Survived']==1):\r\n","            s1+=1    \r\n","        else:\r\n","            d1+=1   \r\n","    elif (df.at[i,'Pclass'] ==2):\r\n","        if(df.at[i,'Survived']==1):\r\n","            s2+=1    \r\n","        else:\r\n","            d2+=1\r\n","    else:  \r\n","        if (df.at[i,'Survived']==1):\r\n","            s3+=1    \r\n","        else:\r\n","            d3+=1         \r\n","    i+=1\r\n","sr1=s1/(s1+d1)\r\n","sr2=s2/(s2+d2)\r\n","sr3=s3/(s3+d3)\r\n","data = {'Pclass1':sr1, 'Pclass2':sr2, 'Pclass3':sr3}\r\n","\r\n","pclass = data.keys()\r\n","rate = data.values()\r\n","fig = plt.figure(figsize = (10, 5))\r\n"," \r\n","plt.bar(pclass, rate, color ='red', width = 0.5, align='center')\r\n"," \r\n","plt.xlabel(\"Passenger Class\")\r\n","plt.ylabel(\"Survival Rate\")\r\n","plt.show()\r\n","\r\n","\r\n","\r\n","\r\n"],"outputs":[],"metadata":{"id":"l3YP8H18si4G"}},{"cell_type":"code","execution_count":null,"source":["sns.kdeplot(df.loc[(df['Survived']==1),\r\n","            'Fare'], color='r', shade=True, Label='1')\r\n","  \r\n","sns.kdeplot(df.loc[(df['Survived']==0), \r\n","            'Fare'], color='b', shade=True, Label='0')\r\n","  \r\n","plt.xlabel('Fare')\r\n","plt.ylabel('Probability Density')\r\n","plt.legend(loc='upper right', title='Survived')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"ekU_aa0Wsn6T"}},{"cell_type":"markdown","source":["## 5. Preprocessing\n","preprocessing is a very important step before we step into modelling"],"metadata":{"id":"NzSBvjqdRfiM"}},{"cell_type":"markdown","source":[" - when we added the Title column in the datadframe, we did preprocessing on the existing data to extract relevant information. As we also tried to explore and understand the data using survival rate, we saw it under EDA."],"metadata":{"id":"1_HLgWAZTD-2"}},{"cell_type":"markdown","source":["### 5.a. Removing unnecessary columns"],"metadata":{"id":"V-eJsxXrTuBJ"}},{"cell_type":"markdown","source":[" - The columns PassengerId and Ticket play no logical role in being related to the survival of a passenger. Hence we will remove them.\n"," - We will also remove the Name column as we have done <b>feature extraction</b> and obtained titles instead. (test set might have extra titles which are not in the train set. Think about how we would consider them. This however doesn't affect task 1's objective)"],"metadata":{"id":"ij1QOguUT7nO"}},{"cell_type":"code","execution_count":null,"source":["#write code to remove unnecessary columns from the dataframe here\r\n","drop_list=['PassengerId', 'Ticket', 'Name']\r\n","df=pd.read_csv(\"train.csv\")\r\n","dropdf=df #making a new drop df so as to not tamper the original df\r\n","dropdf.drop(drop_list, axis=1, inplace=True)\r\n","dropdf"],"outputs":[],"metadata":{"id":"lpLKo4YgMSKA"}},{"cell_type":"code","execution_count":null,"source":["\r\n","\r\n","    \r\n","\r\n","\r\n","    \r\n"],"outputs":[],"metadata":{"id":"IwARq2gEaoYK"}},{"cell_type":"markdown","source":["### 5.b. Nan analysis"],"metadata":{"id":"63jou6PJaoqp"}},{"cell_type":"markdown","source":[" - write code to find the percentage of Nans in each column and visualize it in a tabular format"],"metadata":{"id":"qh6_TT42ay-x"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","\r\n","\r\n","x=0\r\n","nan_list=dropdf.columns \r\n","length=len(nan_list)\r\n","number=[0]*length\r\n","perc=[0]*length\r\n","\r\n","\r\n","for x in range(length):\r\n","    number[x]=df[nan_list[x]].isnull().sum()\r\n","    perc[x]=((number[x])/(rows))*100\r\n","    x+=1\r\n","\r\n","nan_df=pd.DataFrame(perc, nan_list, columns=['Percentage of Nan'])\r\n","nan_df\r\n"],"outputs":[],"metadata":{"id":"F0V-qmTDUMAx"}},{"cell_type":"markdown","source":[" - remove any column having more than 50% Nans as they would be of no use"],"metadata":{"id":"2mqclVHla_ey"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","\r\n","for ind in nan_df.index:\r\n","\tif((nan_df['Percentage of Nan'][ind])>=50):\r\n","\t\tremove=ind\r\n","\r\n","dropdf.drop(remove, axis=1, inplace=True)\r\n","dropdf\r\n","\r\n","\r\n","\r\n"],"outputs":[],"metadata":{"id":"wN6LpxsKa-Zi"}},{"cell_type":"markdown","source":[" - Fill Nans in the Embark column with the statistical mode\n"," - Fill Nans in the Age column with it's statistical mean"],"metadata":{"id":"YfPIbLBkbU8C"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","i=0\r\n","replacedf=dropdf\r\n","\r\n","mode=replacedf['Embarked'].mode()\r\n","replacedf['Embarked'] = replacedf['Embarked'].replace(np.nan, mode[0])\r\n","#print(replacedf['Embarked'].unique())\r\n","mean=replacedf['Age'].mean()\r\n","\r\n","replacedf['Age'] = replacedf['Age'].replace(np.nan, mean)\r\n","#print(replacedf['Age'].unique())\r\n","replacedf\r\n","\r\n"],"outputs":[],"metadata":{"id":"ht5Uxpb6a9SA"}},{"cell_type":"markdown","source":[" - show the new Nan percentage vs column name table after filling and removing to make sure there are no Nans"],"metadata":{"id":"ldZ-7Knyc7r1"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","x=0\r\n","new_list=replacedf.columns \r\n","l=len(new_list)\r\n","no=[0]*l\r\n","per=[0]*l\r\n","\r\n","for x in range(l):\r\n","    no[x]=df[new_list[x]].isnull().sum()\r\n","    per[x]=((no[x])/(rows))*100\r\n","    x+=1\r\n","\r\n","new_nan_df=pd.DataFrame(per, new_list, columns=['New Nan Percentage'])\r\n","new_nan_df\r\n","\r\n"],"outputs":[],"metadata":{"id":"qalNu0Gvc6ih"}},{"cell_type":"code","execution_count":null,"source":["\r\n"],"outputs":[],"metadata":{"id":"_aeWd33AepZi"}},{"cell_type":"markdown","source":["### 5.c. Adding features to the dataset"],"metadata":{"id":"EQKtPBSSgUpN"}},{"cell_type":"markdown","source":[" - Read the kaggle link (titanic dataset) provided above and see what the two columns SibSP and Parch signify\n"," - Using that create a new column called <B>Family_members</b>\n"," - Fill it with the sum of SibSP and Parch"],"metadata":{"id":"daJuxRs4gcH9"}},{"cell_type":"code","execution_count":482,"source":["#write code here\r\n","\r\n","total=df['SibSp']+df['Parch']\r\n","df['Family_Members']=total\r\n"],"outputs":[],"metadata":{"id":"IZF7_Jfagbd_"}},{"cell_type":"markdown","source":[" - Now you may remove the two columns SibSP and Parch<br>\n"," It is important to remove them because they introduce multicollinearity in our dataset now as the new column Family_members is directly dependent on them.<br>\n"," Multicollinearity is deadly to linear models that we will look into later"],"metadata":{"id":"1jrABHyphdWg"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","drop_li=['SibSp', 'Parch']\r\n","df.drop(drop_li, axis=1, inplace=True)\r\n","df"],"outputs":[],"metadata":{"id":"aPf3p6WChc3g"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"ZoC97Umai-vg"}},{"cell_type":"markdown","source":["### 5.d. Scaling and Normalization\n","we don't want to give undue advantage to a few columns (when training models), just because they have a numeric data that is larger in magnitude.<br>\n","Therefore we scale all values down to a specific range or distribution\n","<br>"],"metadata":{"id":"Az401HQIi8FR"}},{"cell_type":"markdown","source":[" - Columns to apply scaling on\r\n","  - Age\r\n","  - Fare\r\n","  - Family_members\r\n","\r\n","- Although Pclass column is numeric, it contains classification data (only 1, 2 and 3). Therefore only continuous data like age, fare, etc needs to be normalized"],"metadata":{"id":"8uXii3c6jx-4"}},{"cell_type":"markdown","source":[" - Read about standardization and normalization in this link https://www.geeksforgeeks.org/normalization-vs-standardization/\r\n"," - apply the formulas given in the link above and scale/normalize the features that are supposed to be scaled\r\n"," - It is your choice to choose wether you choose to normalize or standardize a column. You will be able to make your choice after reading the link contents.\r\n"," - If you feel confused with the choice, then you can go ahead and directly normalize."],"metadata":{"id":"NwDqNCcrlYtR"}},{"cell_type":"code","execution_count":null,"source":["#write code here to normalize data\r\n","\r\n","#Normalisation of Age\r\n","age_maxes = df['Age'].max()\r\n","df1_max = age_maxes.max()\r\n","age_mins = df['Age'].min()\r\n","df1_min = age_mins.min()\r\n","normalized_age = (df['Age'] - df1_min) / (df1_max - df1_min)\r\n","#Normalisation of Family Members\r\n","family_maxes = df['Family_Members'].max()\r\n","df2_max = family_maxes.max()\r\n","family_mins = df['Family_Members'].min()\r\n","df2_min = family_mins.min()\r\n","normalized_family = (df['Family_Members'] - df2_min) / (df2_max - df2_min)\r\n","#Standardised Fare\r\n","fare_maxes = df['Fare'].max()\r\n","df3_max = fare_maxes.max()\r\n","standard_fare = (df['Fare'] / df3_max)\r\n"],"outputs":[],"metadata":{"id":"9LzoSA42h_wv"}},{"cell_type":"code","execution_count":null,"source":["df['Age']=normalized_age\r\n","df['Family_Members']=normalized_family\r\n","df['Fare']=standard_fare\r\n","df\r\n"],"outputs":[],"metadata":{"id":"C88SQvKkm-r8"}},{"cell_type":"markdown","source":["### 5.e. One hot encoding\r\n","Machine Learning models do not understand strings <br>\r\n","They understand numbers<br>\r\n","That is one of the reasons why we one hot encode few columns<br>\r\n","\r\n","- refer this link for more information : https://datagy.io/pandas-get-dummies/\r\n","- you can use this or directly look into pandas get_dummies() method to implement this"],"metadata":{"id":"8KXM4Xe0m-bj"}},{"cell_type":"markdown","source":[" - columns to apply one_hot encoding on\r\n"," - Pclass\r\n"," - Sex\r\n"," - Title\r\n"," - Embarked\r\n"],"metadata":{"id":"1IIsD-KcpVBc"}},{"cell_type":"code","execution_count":null,"source":["#write code here\r\n","#one hot encoding on sex\r\n","sex_enc = pd.get_dummies(df['Sex'])\r\n","#one hot encoding on pclass\r\n","pclass_enc = pd.get_dummies(df['Pclass'])\r\n","#one hot encoding on embarked\r\n","embarked_enc = pd.get_dummies(df['Embarked'])\r\n","#one hot encoding on \r\n","title_enc = pd.get_dummies(df2['Title'])\r\n","\r\n","df=df.join(sex_enc)\r\n","df=df.join(pclass_enc)\r\n","df=df.join(embarked_enc)\r\n","df=df.join(title_enc)\r\n","\r\n"],"outputs":[],"metadata":{"id":"Y60WNgwLjrmn"}},{"cell_type":"code","execution_count":null,"source":["#show the final dataframe here\r\n","df"],"outputs":[],"metadata":{"id":"r4wogy53paiL"}},{"cell_type":"markdown","source":["End of Preprocessing"],"metadata":{"id":"QhGsnsY4puYz"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"Wa1zcX8_pyk5"}}]}